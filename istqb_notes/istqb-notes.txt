--- Fundamentals of Testing ---

1
international software testing qualifications board


2 what is testing
analysis + test design
Test execution, and checking results
bug and test reporting

1. prevent defects - not just meeting requirements, but looking ahead to other issues. making sure requirements make sense (early)
2. find defects
3. secondary react to found issues by users
4. evaluate if requirements met
5. is it meeting what the customers wanted
6. confidence in quality
7. provide information (in regards to quality) (minor vs severe bugs)
8. comply with contractual/legal/etc standards

Testing (show failures/confirming fix) != Debugging (finding the reason and fixing them)


3 why is testing necessary
test techniques @ software lifecycle "appropriate points"
- during requirement review, design, during code, testing/validating before release

QM -> QA &~ QC
QA - adherence to process, prevents, proactive
QC - testing, design, execution - reactive, detect

a Error: mistake in design/requirements
b Defect: bug - built into software
c Failure: software doesnt do what supposed to do, observed

mistakes - pressure, human fallibility, inexperience/skill, complexity, misunderstanding, new-techniques

Root cause vs effects - find root cause


4 seven testing principles
1. testing shows presence of defects, not absence
no bugs found, no issues to reporting
absence of evidence does not evidence of absence

2. exhaustive testing is impossible
combinatorial explosion
test a representative protion vs risk analysis,

3. early testing saves time and money

4. defects cluster together
test most problematic/newest area/logical reasoning

5. pesticide paradox
as existing tests find less bugs, review data + make new tests

6. testing is context-dependent
time, location, interests
security important? (book club vs bank)
must be knowledable specifically industry

7. absence of errors is a fallacy
durability testings
even if many defects caught/fixed, not guaranteed success


5 test process fundamentals
testers also make mistakes
no single correct test process - different approaches
context has lots of influence, plus internal things
similar activities
7 principles
planning - techniques, schedules, deadlines
monitoring+control - plans vs actual, looping between steps, revising
analysis - tests @ moment, looking at reports - revise
test design and implementation - identify and prioritize test - HOW being tested - putting tests into place
test execution (manual or auto)
test completion (retrospective, archived)

>> work products
 - test plan, progess reports, summaries, traceable test cases, summary report
traceability = keeping track of test cases, logging, governance, recording, keep for a certain time.

6 test psychology
big bad tester: reports bad news only. dev gets told they made mistake
the critic: overly critical ("again", timing of how often)

constructive, clear, informative bug reports - what when how - version, requirements, description, steps, expected result, actual result, screenshots/etc
confirm understanding - soft skills

confirmation bias
no bugs until proven vs always has bugs
devs dont want more work, tester thinks otherwise








--- Testing throughout the Software Development Lifecycle ---

-- Model types:
SDLC - systematic plan
basis for planing control, knows role, costs/speed -> smooth progress, faster dev, cost low
"a model  describes types of activity at each stage in project, how relate to each other logically and chronologically"

Sequential:
waterfall -> fully sequential = bad model tbh
v model -> testing during development - early testing - different test level with each dev phase

Iterative:
time cycles, weeks, one group working on one while another doing another - small features per X weeks?
Rational Unified Process (longer)
Agile, Scrum (shorter)
Kanban (short to long, but more fluid than agile)
spiral (experimental, very flexible)

-- Test Levels:
like adding puzzle pieces together - 4 levels
component (aka unit or module)(by devs), integration (two+ components/units)(subsystems, databases, infrastructure, microservices), system (whole, with everything together, with other stuff)(starts outside the system, end point, car anology), acceptance(end user UAT, OAT operation acceptance testing (backup/restore/load/performance) (system fits what customer wanted))
objectives of testing: reduce risk, verify functional, confidence in system, find bugs, prevent defects @ higher levels

Alpha = on dev
beta = on user environment (commercial off the shelf COTS)


--Test Types
Functional:
feature function right, as expected.
unit, integration, [system, ui]-acceptance testing
Coverage: % to which an application is exercised by tests
Coverage-Gap: what isnt tested yet, %
Non Functional:
usability, performance, security
UI, server load, stress test, endurance test, secure - pen test, white-hat hacking

whitebox: understands what is going on in systems, using technical docs to verify specific subsystem
blackbox: don't know about software internals, only use Reqs to send input more or less as user, timings

change-related testing:
confirmation testing: was the bug fixed
regression testing: did the fix break something else (automated is best)
bug fix, new features, change to exisintg features, config/environment changes(changes not to app)

can be multiple type of test at once, but one focus. any test type at any level

Maintenance Testing:
maintained over time. 
Unplanned: bugs or failures (hotfix)(security)
Planned: enhancements, operation/environment upgrades (new version of OS/program), retirement (sw unfit, needs new tech), new hardware/services

Impact Analysis:
What changes, where, which parts affected, likely affected
needs new tests or modify old tests






--- Static Testing ---
-- essentials
testing before coding to an extent, reviewing requirements, before something goes wrong, tool driven evaluation of work products with structure. security testing.
business req, architecture, user stories, code, web pages, user guides
early bug finding easier and cheaper to remove, static testing = upfront investment


Static/dynamic = sameish objectives
- assess quality, identify defects, but find different type of defects
inefficient algorithms, database structures, cohesion
static: coding standards, incorrect interface specifications, security vulnerabilities, gaps in test coverage

-- Review Process
find issue that might lead to defects
potential defect / recommendation / question
Informal - no defined process, no documents
Formal - team participation, official flow, documentation
4 review types = informal, walkthrough (find defects), technical review, inspection (review work product)
planning review: 30%, scope? - purpose, what review, quality characteristics - review type, activities, roles, checklists
Size of document <> checking rate <> hours spent
entry/exit criteria - is it ready? - less than X defects per Y
individual review: question, missing, recommendations
issue analysis - accept, accept w/ changes,reject major needed
communicate defects, assign owner ship, evaluate, make decision
fixing and reporting: create defect reports, fix defects, communicate defects. update defect status, gather metrics, exit creteria met, accept work product
review process: plan, initiate, prepare, meet, fix
Author: creates work product, addresses found issues
Review Leader: overall responsibility, participants, schedule
Facilitator: runs meeting, mediates viewpoints
Reviewers: technical, project staff, business expert (functionality, usability)
Scribe: collate issues found, records new issues
Management: overall review planning, review strategy, staff budget/time, cost effective, control decisions
lfarms

-- Review Types
Informal Reviews - agile, detect defects, generate ideas, buddy dev, no process, limited docs
Walkthrough - author prepares and presents, aim to improve product, technique, style - large audience, low participant demand - important issues found, scenarios, dry runs, simulators. find defects, improve. formal to informal, maybe checklists. logs/defects
technical reviews - gain consensus, mix of small group with varied background. formal - plan, initiate, individual review (prepare), issue communcation, fix + report
software inspection - very defined process. small group, multi roles, peers author or tech expert. checklist checking. 
higher risk = more formal/technical

-- Applying Review Process
Have clear objects, choose appropriate review types, suitable review techniques
chunking for large documents, adequate notice + time to prepare, management support (upfront investment)
manager: involve right people in review, values testers as reviewers, attention to detail, concenrtation
people related: defects acknolwedged (dont take it personal), well managed, atmosphere, training
review techniques:
adhoc - informal, one off, little prep, rely on reviewer expertise, little guidance, sequential read, duplicate issues common
checklist-based - checklist provided to reviewers based on previous things, systematic, typical defects covered, but should look outside of objectives. derived from rules
scenarios/dryruns - structured guidelines on reading based on expected usage, better than simple checklists, but shouldnt feel contrained, create paths of use
role-based - experienced user, novice user, developers/testers - basically act as a certain end user
perspective-based (reading) - software reading, devs learn gradually, less reliant on individual, create work products. concrete instructions to uncover defects










--- Test Techniques ---
selecting technique:
time&budget, (experience = easier, cheaper, lest formal, but might miss)
regulatory/contracts = documentation
available documentation
tester knowledge/skill
sdlc model (formality), system complexity (more varied techniques)


-- Black Box
- basis: user stories, use cases, req documents
- test cases: detect deviations between req and implementation
- coverage: based on items tested as well as technique applied
- not performed at component level
based on requirements, might change req. static and dynamic.
focus on only input/output, not knowing internals

Equivalence Partitioning:
"all values in an equivalence partition should processed in same way" in 5-100, need 4 95 101, 1 value per partition.
less time taken, but less checking. not always feasible to check. number line good for idea of checking
- reduce test case, all levels, coverage by number of equivalence paritions tested, partitions identify any element related. invalid partitions not tested at same time.

Boundary Value Analysis:
Ordered data only. extends Equivalence Partitioning but checks edge cases
Lowest/highest per boundary area.
2 point analysis - around boundary
3 point analysis - includes the boundary itself

- defects more likely to occur near boundaries. coverage = boundary values / boundary tests. all test levels. must be ordered/numeric data


Decision Table Testing:
Condition 1
Condition 2
Condition 3
Action

Wide variety. actions = observable results. start with most negative
display complex logic, finds many conditions, enough test cases to cover every combination of conditions. coverage = decision rules tested . decision rule = particular combination of conditions
all test levels



State Transition Testing:
4 components: states software is in, transitions (between states), events (causes transition), actions
only shows valid transitions
State Transition table: like decision table, but between states
used for menu apps/screen navigiation - bad for large systems with non sequential inputs


Use Case Testing:
Use case: specific way of interacting with a system
blends with acceptance tests
Preconditions: conditions met for use case test to work successfully, ie logged in, assigned class
post conditions= conditions met after use case test completed, ie email notificaiton, records

Actors: users, extenral hardware, other systems (api)
Subject: component/test object that is interacted with
Use case scenarios: correct but also wrong/varied cases, like unassigned or server down, ie. exceptional scenarios


-- White Box
- basis: includes source of info regarding structure
- performed by devs
- based on number of items tested
- performed at all levels (mostly component) (dyn/static)

Statement Testing:
decision points @ if statements.
executable statement: thing that happens
statement test = exercises all executable statements
flow charts good

Decision Testing:
exercises all decision points instead of the statements. finds outcome of each.
100% statement cov != 100% decision cov (usually)



-- Experience Based
- past experience of: tester, dev, user, stakeholders
- expected use of product (ie children)
-- basis knowledge of many
-- expected use + likely defects
-- combined with white-box/black-box dynamic


Error Guessing:
creating tests to expose failures that are likely to occur - attempts to predict occurence of defects. more methodically using lists
ie touchscreen for kids
contradictory system requirements/documentation defects
new technologies tester may know
integration with 3rd party services
cache issues

Error Guessing Data Source:
Past experience, general causes of software failure, defect & failure data from previous projects


Checklist based testing:
checklists can result in higher test coverage
less specific checklist can increase variability depending on testing, lower repeatability
past experience, general causes of failure, defect/failure data, user data (performance)

- checklists define test conditions
- checklists may be reused (with modifciation)
- various test types
- increase test coverage, reduce repeatability)

Exploratory Testing:
informal
tests designed, executed, logged, evaluated during execution - documented
explore -> document -> review -> ...

- Session based: defined time box, test charter guiding, activities documented in testing sheets

Useful: limited documentation (since less formal), time and budget constraints, best combined with other techniques

- not predefined tests
- commonly used in reactive test strategies
- makes use of time boxed sessions








--- Test Management ---
-- The Test Manager
- writes the rest plan
- no test: customer complaints, bottom line, damage to reputation
Test Policy: why/what test - test process, how evaluated, quality levels, process improvement - early activities, estimates of effort, test on mirror of production env
sapce - scope, people, approach, criteria, environment 
Test Plan: how test - test objectives
1 analytical - risk based
2 model based - design on model, growth model, business model, state model
3 methodically - predefined tests, common failure types, checklist of quality characteristics
4 process compliant - tests based on external/industry standards, process documentation
5 directed (consultative) - advice from experts, external to test team
6 regression averse - reuse test are, auto tests, test suites
7 reactive - little preplanning, simultaneous design and execution, exploratory testing

Estimating:
When start testing? early as possible.
Estimating
prepare analysis and design, writing
implementation, automate test
evaluate results, bugs
test levels, test cycles

schedule test activities, coordinate product owner, manager
write test plan, keep updated "living document"

Where test: proper test environment i.e. production equivalent, or at the desk
- prod - problems recreated, performance realistic, live cust dater BUT expensive, user license, data sanatized

how test carried out:
manual + automated, acquire resources (right people), support tool selection
defect management system, configuration management (server settings, version control), metrics of testing
progress monitored, exit criteria, + report, continually adapt



-- The Tester
Analyze, review, assess test basis. focus on X. document test conditions. capture traceability
design test, reserve equipment, coordinate with sys admin
identify test conditions
design and implement test cases/procedures, build test DB, prepare data
create schedule, execute tests + document/tickets, use correct tools, automate what they can
ISO/IEC Standard 25010 - describes a classification of software product quality characteristics
Performance, Efficiency, reliability, compatibility, usability, portability, + security
roles: technical test analyst, test automation engineer, performance tester, security tester
OAT - operational acceptance (testing)
UAT - user acceptance
ST - system testing - independent
unit testing - devs
Independent testing: can find defects in own work, degree of tester independence, but doesnt replace familiarity 
1: devs own code
2: some independent testers
3: independent test team
4: business testers (pro)
5: external, independent testers (outsource/insource)

Pro:
recognize different failures, varify/challenge/disprove assumptions
Con:
isolation from devs, lose responsibility for quality, testers seen as bottleneck, testers lack critical info


-- Product Risk Analysis
Type of risk:
Product Risk:
Software not performing to specification, ignores customer needs, bad system architecture, wrong computations, loops coded incorrectly, slow response times, poor user experience
product risk = quality risk

Project risk (internal): 5 (project issues, organizational, political, technical, supplier)
project issue (delay in delivery), inadequate funding, late changes, organizational issues (project resourcing (new software), personal issues, conflicting business priorities (people on multiple projects))
office politics, bad attitude towards testing, failure to improve testing practices, poor communication of testing
technical issues: poor requirement, existing constrains, test env not ready, data migration late, variably quality of work products, technical debt (accumulated defects)
supplier issues: 3rd party fails to deliver, contractual issues

Getting back on track:
- test manager handles, hire contractors, work overtime, improve estimating, manage changes better


Risk based testing:
Impact: harm caused if software fails, from 1 to 5, all stakeholders provide input
Likelihood: chance of failure, 1 to 5, technical reasons prevail
Risk level of req = impact x likelihood
(remember airline example)

Risk: helps focus effort
Adverse events less likely, impact reduced
testing = risk mitigation
pro-active strategy
choose test techniques, select levels/types of testing, depth of testing, prioritize testing, additional activities required


-- Managing Defects
How to write defect report:
- like 20 different terms for flaw/bug/defect
- error -> fault/defect/bug -> failure "Defect is a manifestation of an error in the software"
finding defects is a test objective. defect reports aid resolution
start new log, analysis of why happened (was data/server problem, or actually bug), track, confirm fix
defects found: during coding, static analysis, dynamic testing, or during product use
defects in: code, software, documentation
defect report: provides information to recreate/resolve, feedback on quality, catalyst for improvement
defect report: id, title, summary, date, organization, author, id of test item, status (new/open/closed/etc), lifecycle phase, test env, detailed desc (expected/actual results), change history references, global issues, conclusions/recommendations/approvals
new -> open -> fixing -> ready -> tested -> closed + resolution info --- (reopen, might recycle)


-- How to write a test plan
Purpose of test plan: test policy (why test), test strategy (what test), test plan (how test)
development lifecycles and methods, scope of testing, test objectives, risks, constraints, testability (ease), availability (testers, hardware)
master test plan -> unit test, system test, uat plan
1 scope, 2 approach, 3 integration, 4 who/what/how, 5 scheduling, 6 metrics, 7 budgeting, 8 templates
test planning: contiuous: initial draft issued, feedback, test plan updated

Test Stragtegy+Approach (space vs financial example)
See line 362 for types
combine strategies

-- Entry/Exit Criteria
Entry: to start
definition of ready
stories/models/strategies, test env ready (database, software ready)

Exit: Definition of done
planned test done, coverage level done, unresolved defects, remaining defects low, level of quality characteristics 
- might end early - budget, schedule, product to market


Applying Estimation Techniques:
Ideal: High, Medium, Low Priority - however, might be dependencies + balance efficiency

Test Effort Influence Factors (4):
Product - risks, test basis quality, size/complexity, quality characteristics, documentation, legal/regulatory/contracts
People - skills, knowledge, experience, cohesion/leadership
Development - organizational maturity, dev model, test approach, tools used, test process (like reviews), time pressure
Test results - number + severity of defects, amount of rework required

Test Estimation Techniques:
longer than you expect
- metrics based estimation - burndown chart, defect removal model
- expert based estimation - planning poker (Fibonacci), wideband delphi (group of experts, lots of interaction - 3 to 7 people chosen, kick off meeting, preparing, estimation session, assemble tasks, review results)
Include all test activities, test preparation not just execution, number of test cycles
uncertainty: agile teams build first, present range + confidence, incremental funding



-- Keeping track of progress:
Test Monitoring:
Metrics: Work done, case execution, detect info, coverage (code, user stories, reqs), task completion, costs
metrics used in: test approach, quality, progress
gather metrics and make visible

Test Control:
Think of air traffic controller
Reprioritize testing, change schedule, add more resources

Test Reporting:
Progress Report
Summary Report

- summary of whats performed, information on occurances, deviations of planes, status of testing/quality of product, factors that block progress, metrics, residual risks, reusable test work products
Also: status of activities, factors of block removed, planning testing for future, quality of test object 

Tailor report to audience

Severity - impact on business
business - how soon to fix (may not be as easy, or available to fix right away)








- Tool Support for Testing
Tool helps in testing process - tool = do math, automate repetitive/significant effort tasks - support manual activities - some things humans do better
more consistent testing, higher level of reproducibility
some activities cannot be executed manually

Probe Effect: the effect on the system by measurment instrument when component or system being measured - tool might slow down program while running, taking data, or maybe doing coverage reports

Classing tool:
Pricing - subscription, perpetual
License - opensource, commercial

- Management of testing
testware - all artifacts produced during test process
management of testing process - test planning, exeuction, etc
management of testing - just the tests
management of tests/activities, scheduling text execution and logging results, linking tests/results/defects to requirements or other sources, creating reports from metrics
often have interface to other tools like bug tracking or code management
requirement management tools - req <--> test cases, track coverage, find ambiguities
defect management tools - desc, severity, env, etc
configuration mngmnt - git
CI tools - jenkins (more dev oriented) - quick feedback about quality of product

- static testing
manual review, tool evaluation of code
static analysis tools - by devs - prevent runtime errors, enforce coding standards, find security gaps, checking code as typed
quality gate

- test design and implementation
model based testing - model based on project requirements (flowcharts) - lots of test cases, find gaps in reqs, assist in automation
data prep tools - need to use unique data andor large amount of data - randomize, generate, anonymous data - good for load test

- test execution
test execution tools - record exact steps, add assertions, replay. hard to maintain, little app control. better to use more in depth programming one.  logging, execution values, reports
coverage tools in here
test harnesses - stubs, drivers, replace part of system (devs)

- performance measurement/dynamic analysis
testing tool = everything that can help
generate load and simulate scenarios, measure response time, find bottlenecks
dynamic analysis - memory leaks, time dependencies, pointer arithmetic errors, require code to be running

- specialized testing needs
non functional characteristics - embedded system performance testing / dynamic analysis tools for security issues



-- Benefits/Risks of Test Automation
benefits - time reduced, coverage increased, reduces manual work, greater consistency/repeatability, objective assessment, easier access to testing info

Risks - expectations unrealisitic, time/cost/effort + time/effort for benefit + effort to maintain - all may be underestimated
      - may be replied upon too much, version control neglected, issues between tools neglected, tool vender out of business or bad support, opensource abandoned, new tech not supported, no clear owner of tool

	  

-- Selecting tools
assese organization maturity, evaluate against requirements/objectives, identify reqs/plan internal implementation
- identify where tool will support, like safety critical
- estimation of costbenefit ratio
- evaluation of vender or open source network
- additional pros/cons, trial period
- compatibility of tool with test object, integration, training needs

test capture tool methods:
- capturing - good stating point, no programming knowledge / cannot scale, hard coded, unstable when unexpected event, high maintenance
- datadriven - script draws from data. highly varied. programming, high effort, well managed
- keyword - datafiles used in control script to use support scripts, additional tests easily added, test scripts added by non programmings, tool independent automated tests



-- Introducing tools
- more comprehensive selection criteria, small scale study to understand tool

pilot project:
proof of concept, varies, set objectives ie can solve a problem?
gain in dpeth knowledge of tool, evaluate how fits, standards use/maintaining tool, reasonable cost of benefits, understanding metrics/reports desired by tool
if objectives not met: not being used right, or not right

-success factors of tool: rolling out incrementally, adapting/improving processes to fit within tool, provide training/coaching/mentoring for tool users, defining use guidelines
- way to gather info, monitor use/benefits, support to user






Other notes:
"Nonfunctional Requirements (NFRs) define system attributes such as security, reliability, performance, maintainability, scalability, and usability. They serve as constraints or restrictions on the design of the system across the different backlogs."

Function:
solutions defined by user stories. basically the exact cases to be covered

why is usability testing non-functional? while it may do XYZ, it may be difficult/slow/annoying/unintuitive


use case 3 outcomes
https://www.softwaretestinghelp.com/use-case-testing/



https://docs.google.com/spreadsheets/d/1UJtBeg4DZQJWaSWwPA_2Iwf6sQoNv5UoNOLWoeRcNeo/edit#gid=0


















